# [MXML-6-08] 10.multiclass(OvR).py
# Implement multiclass classification of SVM by One-Rest (OvR)
# Since SVC operates as an OvO internally, we will use 
# OneVsRestClassifier.
#
# This code was used in the machine learning online 
# course provided by 
# www.youtube.com/@meanxai
# www.github.com/meanxai/machine_learning
#
# A detailed description of this code can be found in
# https://youtu.be/ogFZchEqmTA
# 
import numpy as np
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Generate the data with 4 clusters.
x, y = make_blobs(n_samples=400, n_features=2, 
                centers=[[0., 0.2], [0.5, 0.5], [1., -0.2], [0.3, -0.3]], 
                cluster_std=0.15)

# Linear SVM model
C = 1.0
model = OneVsRestClassifier(SVC(C=C, kernel='linear'))
model.fit(x, y)

print(model.estimators_)
# [SVC(kernel='linear'),
#  SVC(kernel='linear'),
#  SVC(kernel='linear'),
#  SVC(kernel='linear')]

w = np.array([m.coef_[0] for m in model.estimators_])      # (4,2)
b = np.array([m.intercept_[0] for m in model.estimators_]) # (4,)

# Visualize the data and 4 boundaries.
plt.figure(figsize=(8,7))
colors = ['red', 'blue', 'green', 'black']
y_color= [colors[a] for a in y]
for label in model.classes_:
    idx = np.where(y == label)
    plt.scatter(x[idx, 0], x[idx, 1], s=100, c=colors[label], 
                alpha=0.5, label='class_' + str(label))

# Visualize 4 boundaries.
x1_dec = np.linspace(-2.0, 2.0, 50).reshape(-1, 1)
for i in range(w.shape[0]):
    x2_dec = -(w[i, 0] * x1_dec + b[i]) / w[i, 1]
    plt.plot(x1_dec, x2_dec, label=str(i)+'_rest')
plt.xlim(-0.5, 1.5)    
plt.ylim(-0.7, 1.)
plt.legend()
plt.show()

# Predict the classes of the test data.
x_test = np.random.uniform(-1.5, 1.5, (2000, 2))
y_pred1 = model.predict(x_test)

# To understand how OvR works, let's manually implement the 
# process of model.predict(x_test). df.shape = (2000, 4)
df = np.dot(x_test, w.T) + b          # decision function
# df = model.decision_function(x_test) # same as above

y_pred2 = df.argmax(axis=1)

# Compare y_pred1 and y_pred2.
if (y_pred1 != y_pred2).sum() == 0:
    print("# y_pred1 and y_pred2 are exactly the same.")
else:
    print("# y_pred1 and y_pred2 are not the same.")

# Visualize test data and y_pred1
plt.figure(figsize=(8,7))
y_color= [colors[a] for a in y_pred1]
for label in model.classes_:
    idx = np.where(y_pred1 == label)
    plt.scatter(x_test[idx, 0], x_test[idx, 1], 
                s=100,
                c=colors[label], 
                alpha=0.3, 
                label='class_' + str(label))

plt.xlim(-1.5, 1.8)    
plt.ylim(-0.7, 1.)
plt.show()

# decision_function_shape = 'ovr' in SVC
# model2 = SVC(C=C, kernel='linear', decision_function_shape='ovr')
# model2.fit(x, y)

# # w and b are generated by OvO method.
# print("w:\n", model2.coef_)       # (6,2)
# print("b:\n", model2.intercept_)  # (6,)

# df2 = model2.decision_function(x_test)
# y_pred3 = df2.argmax(axis=1)

# # Visualize test data and y_pred3
# plt.figure(figsize=(8,7))
# y_color= [colors[a] for a in y_pred3]
# for label in model.classes_:
#     idx = np.where(y_pred3 == label)
#     plt.scatter(x_test[idx, 0], x_test[idx, 1], s=100,
#                 c=colors[label], 
#                 alpha=0.3, label='class_' + str(label))

# plt.xlim(-1.5, 1.8)    
# plt.ylim(-0.7, 1.)
# plt.show()

